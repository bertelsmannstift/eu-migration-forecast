{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# notebooky stuff\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import display\n",
    "\n",
    "import sys \n",
    "sys.path.append('../../modules')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import eumf_data, eumf_eval, eumf_pipeline, eumf_custom_models\n",
    "from sklearn import dummy, preprocessing, linear_model, svm, model_selection, ensemble, gaussian_process\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "import xgboost as xgb\n",
    "import json\n",
    "\n",
    "\n",
    "# pandas pretty output\n",
    "pd.set_option('display.min_rows', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stei509/eu_migration_forecast/notebooks/final_paper/../../modules/eumf_data.py:238: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index)\n",
      "/home/stei509/eu_migration_forecast/notebooks/final_paper/../../modules/eumf_data.py:279: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df.index = pd.to_datetime(df.index)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"18\" halign=\"left\">10</th>\n",
       "      <th colspan=\"18\" halign=\"left\">11</th>\n",
       "      <th colspan=\"14\" halign=\"left\">112</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"14\" halign=\"left\">value</th>\n",
       "      <th colspan=\"18\" halign=\"left\">gdp</th>\n",
       "      <th colspan=\"18\" halign=\"left\">unempl</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>AT+CH</th>\n",
       "      <th>BE+NL+LU</th>\n",
       "      <th>BG</th>\n",
       "      <th>CZ+SK</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SE+FI+DK</th>\n",
       "      <th>SI</th>\n",
       "      <th>AT+CH</th>\n",
       "      <th>BE+NL+LU</th>\n",
       "      <th>BG</th>\n",
       "      <th>CZ+SK</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SE+FI+DK</th>\n",
       "      <th>SI</th>\n",
       "      <th>AT+CH</th>\n",
       "      <th>BE+NL+LU</th>\n",
       "      <th>BG</th>\n",
       "      <th>CZ+SK</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>...</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SE+FI+DK</th>\n",
       "      <th>SI</th>\n",
       "      <th>AT+CH</th>\n",
       "      <th>BE+NL+LU</th>\n",
       "      <th>BG</th>\n",
       "      <th>CZ+SK</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SE+FI+DK</th>\n",
       "      <th>SI</th>\n",
       "      <th>AT+CH</th>\n",
       "      <th>BE+NL+LU</th>\n",
       "      <th>BG</th>\n",
       "      <th>CZ+SK</th>\n",
       "      <th>ES</th>\n",
       "      <th>FR</th>\n",
       "      <th>GB</th>\n",
       "      <th>GR</th>\n",
       "      <th>HR</th>\n",
       "      <th>HU</th>\n",
       "      <th>IE</th>\n",
       "      <th>IT</th>\n",
       "      <th>LV+LT+EE</th>\n",
       "      <th>PL</th>\n",
       "      <th>PT</th>\n",
       "      <th>RO</th>\n",
       "      <th>SE+FI+DK</th>\n",
       "      <th>SI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-03-31</th>\n",
       "      <td>10.524</td>\n",
       "      <td>16.048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.667</td>\n",
       "      <td>21.857</td>\n",
       "      <td>21.381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.714</td>\n",
       "      <td>60.190</td>\n",
       "      <td>2.190</td>\n",
       "      <td>10.857</td>\n",
       "      <td>38.714</td>\n",
       "      <td>29.810</td>\n",
       "      <td>53.381</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.619</td>\n",
       "      <td>11.095</td>\n",
       "      <td>22.048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.952</td>\n",
       "      <td>12.714</td>\n",
       "      <td>11.571</td>\n",
       "      <td>26.524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.190</td>\n",
       "      <td>178.952</td>\n",
       "      <td>42.238</td>\n",
       "      <td>127.048</td>\n",
       "      <td>33.143</td>\n",
       "      <td>63.333</td>\n",
       "      <td>47.286</td>\n",
       "      <td>9.000</td>\n",
       "      <td>63.095</td>\n",
       "      <td>62.810</td>\n",
       "      <td>59.333</td>\n",
       "      <td>14.905</td>\n",
       "      <td>179.429</td>\n",
       "      <td>39.524</td>\n",
       "      <td>...</td>\n",
       "      <td>1689.000</td>\n",
       "      <td>1268.333</td>\n",
       "      <td>1080.333</td>\n",
       "      <td>2327.333</td>\n",
       "      <td>4459.000</td>\n",
       "      <td>3747.667</td>\n",
       "      <td>156.000</td>\n",
       "      <td>5255.333</td>\n",
       "      <td>1396.333</td>\n",
       "      <td>12264.667</td>\n",
       "      <td>866.333</td>\n",
       "      <td>18186.333</td>\n",
       "      <td>654.333</td>\n",
       "      <td>420.333</td>\n",
       "      <td>28780.0</td>\n",
       "      <td>42970.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>7620.0</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>8450.0</td>\n",
       "      <td>9080.0</td>\n",
       "      <td>3760.0</td>\n",
       "      <td>2590.0</td>\n",
       "      <td>2820.0</td>\n",
       "      <td>14640.0</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>10490.0</td>\n",
       "      <td>2760.0</td>\n",
       "      <td>4520.0</td>\n",
       "      <td>1850.0</td>\n",
       "      <td>33780.0</td>\n",
       "      <td>4790.0</td>\n",
       "      <td>2.85</td>\n",
       "      <td>6.433</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.00</td>\n",
       "      <td>18.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>22.4</td>\n",
       "      <td>12.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7.4</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7.333</td>\n",
       "      <td>5.3</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5.3</td>\n",
       "      <td>7.167</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-30</th>\n",
       "      <td>4.952</td>\n",
       "      <td>14.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.048</td>\n",
       "      <td>13.952</td>\n",
       "      <td>18.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.286</td>\n",
       "      <td>61.619</td>\n",
       "      <td>10.619</td>\n",
       "      <td>7.714</td>\n",
       "      <td>24.381</td>\n",
       "      <td>27.952</td>\n",
       "      <td>55.714</td>\n",
       "      <td>4.714</td>\n",
       "      <td>2.333</td>\n",
       "      <td>7.095</td>\n",
       "      <td>20.619</td>\n",
       "      <td>17.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.810</td>\n",
       "      <td>23.143</td>\n",
       "      <td>11.143</td>\n",
       "      <td>32.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.619</td>\n",
       "      <td>168.905</td>\n",
       "      <td>35.190</td>\n",
       "      <td>83.429</td>\n",
       "      <td>23.810</td>\n",
       "      <td>57.857</td>\n",
       "      <td>48.000</td>\n",
       "      <td>11.905</td>\n",
       "      <td>33.048</td>\n",
       "      <td>27.905</td>\n",
       "      <td>62.810</td>\n",
       "      <td>14.286</td>\n",
       "      <td>126.667</td>\n",
       "      <td>21.476</td>\n",
       "      <td>...</td>\n",
       "      <td>1427.667</td>\n",
       "      <td>1023.333</td>\n",
       "      <td>1049.000</td>\n",
       "      <td>2113.333</td>\n",
       "      <td>4121.333</td>\n",
       "      <td>3637.667</td>\n",
       "      <td>164.667</td>\n",
       "      <td>4699.000</td>\n",
       "      <td>1407.000</td>\n",
       "      <td>12940.000</td>\n",
       "      <td>609.667</td>\n",
       "      <td>19264.000</td>\n",
       "      <td>520.333</td>\n",
       "      <td>365.000</td>\n",
       "      <td>29250.0</td>\n",
       "      <td>44540.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>6330.0</td>\n",
       "      <td>8560.0</td>\n",
       "      <td>8980.0</td>\n",
       "      <td>4120.0</td>\n",
       "      <td>2990.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>14700.0</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>11780.0</td>\n",
       "      <td>2960.0</td>\n",
       "      <td>4790.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>35590.0</td>\n",
       "      <td>5260.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.867</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.75</td>\n",
       "      <td>17.3</td>\n",
       "      <td>9.6</td>\n",
       "      <td>4.4</td>\n",
       "      <td>21.8</td>\n",
       "      <td>11.5</td>\n",
       "      <td>4.3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7.667</td>\n",
       "      <td>5.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>7.133</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>4.905</td>\n",
       "      <td>16.095</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.429</td>\n",
       "      <td>21.048</td>\n",
       "      <td>16.048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.762</td>\n",
       "      <td>68.952</td>\n",
       "      <td>16.095</td>\n",
       "      <td>8.143</td>\n",
       "      <td>20.476</td>\n",
       "      <td>33.762</td>\n",
       "      <td>56.000</td>\n",
       "      <td>6.762</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.095</td>\n",
       "      <td>21.333</td>\n",
       "      <td>29.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.048</td>\n",
       "      <td>8.286</td>\n",
       "      <td>11.190</td>\n",
       "      <td>30.619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.143</td>\n",
       "      <td>171.190</td>\n",
       "      <td>32.143</td>\n",
       "      <td>75.905</td>\n",
       "      <td>21.810</td>\n",
       "      <td>60.381</td>\n",
       "      <td>42.952</td>\n",
       "      <td>11.429</td>\n",
       "      <td>28.667</td>\n",
       "      <td>28.048</td>\n",
       "      <td>67.143</td>\n",
       "      <td>16.476</td>\n",
       "      <td>145.048</td>\n",
       "      <td>25.571</td>\n",
       "      <td>...</td>\n",
       "      <td>2495.333</td>\n",
       "      <td>1796.333</td>\n",
       "      <td>1691.667</td>\n",
       "      <td>2704.000</td>\n",
       "      <td>5043.000</td>\n",
       "      <td>4514.000</td>\n",
       "      <td>283.333</td>\n",
       "      <td>5486.000</td>\n",
       "      <td>1742.667</td>\n",
       "      <td>14442.333</td>\n",
       "      <td>743.667</td>\n",
       "      <td>20279.000</td>\n",
       "      <td>960.000</td>\n",
       "      <td>388.000</td>\n",
       "      <td>28610.0</td>\n",
       "      <td>43130.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>8810.0</td>\n",
       "      <td>6180.0</td>\n",
       "      <td>8400.0</td>\n",
       "      <td>8650.0</td>\n",
       "      <td>4420.0</td>\n",
       "      <td>3380.0</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>16280.0</td>\n",
       "      <td>7120.0</td>\n",
       "      <td>12240.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>34270.0</td>\n",
       "      <td>5330.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>5.733</td>\n",
       "      <td>6.1</td>\n",
       "      <td>5.30</td>\n",
       "      <td>16.8</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7.067</td>\n",
       "      <td>4.7</td>\n",
       "      <td>8.7</td>\n",
       "      <td>4.8</td>\n",
       "      <td>7.033</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-31</th>\n",
       "      <td>4.667</td>\n",
       "      <td>18.905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.190</td>\n",
       "      <td>14.143</td>\n",
       "      <td>16.762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.429</td>\n",
       "      <td>42.619</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1.952</td>\n",
       "      <td>17.476</td>\n",
       "      <td>36.667</td>\n",
       "      <td>45.524</td>\n",
       "      <td>1.810</td>\n",
       "      <td>2.190</td>\n",
       "      <td>3.333</td>\n",
       "      <td>23.286</td>\n",
       "      <td>26.667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.429</td>\n",
       "      <td>12.381</td>\n",
       "      <td>9.667</td>\n",
       "      <td>22.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.048</td>\n",
       "      <td>146.714</td>\n",
       "      <td>37.381</td>\n",
       "      <td>104.048</td>\n",
       "      <td>29.143</td>\n",
       "      <td>54.810</td>\n",
       "      <td>38.762</td>\n",
       "      <td>10.476</td>\n",
       "      <td>46.333</td>\n",
       "      <td>40.667</td>\n",
       "      <td>59.048</td>\n",
       "      <td>19.429</td>\n",
       "      <td>157.095</td>\n",
       "      <td>27.857</td>\n",
       "      <td>...</td>\n",
       "      <td>1842.333</td>\n",
       "      <td>1358.667</td>\n",
       "      <td>1138.667</td>\n",
       "      <td>2784.667</td>\n",
       "      <td>3961.333</td>\n",
       "      <td>3252.667</td>\n",
       "      <td>167.000</td>\n",
       "      <td>4942.000</td>\n",
       "      <td>1482.333</td>\n",
       "      <td>9845.667</td>\n",
       "      <td>570.000</td>\n",
       "      <td>15346.333</td>\n",
       "      <td>564.333</td>\n",
       "      <td>348.000</td>\n",
       "      <td>29160.0</td>\n",
       "      <td>46720.0</td>\n",
       "      <td>2070.0</td>\n",
       "      <td>9010.0</td>\n",
       "      <td>6520.0</td>\n",
       "      <td>8850.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>4170.0</td>\n",
       "      <td>2950.0</td>\n",
       "      <td>3590.0</td>\n",
       "      <td>16920.0</td>\n",
       "      <td>7640.0</td>\n",
       "      <td>12460.0</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>4860.0</td>\n",
       "      <td>2850.0</td>\n",
       "      <td>36230.0</td>\n",
       "      <td>5440.0</td>\n",
       "      <td>2.70</td>\n",
       "      <td>5.367</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.10</td>\n",
       "      <td>16.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>20.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6.767</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.800</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-31</th>\n",
       "      <td>4.429</td>\n",
       "      <td>10.762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.905</td>\n",
       "      <td>18.762</td>\n",
       "      <td>20.429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.000</td>\n",
       "      <td>36.333</td>\n",
       "      <td>8.714</td>\n",
       "      <td>7.952</td>\n",
       "      <td>25.810</td>\n",
       "      <td>25.619</td>\n",
       "      <td>52.286</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.714</td>\n",
       "      <td>3.857</td>\n",
       "      <td>23.143</td>\n",
       "      <td>33.476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.667</td>\n",
       "      <td>34.714</td>\n",
       "      <td>5.429</td>\n",
       "      <td>28.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.000</td>\n",
       "      <td>174.667</td>\n",
       "      <td>48.333</td>\n",
       "      <td>140.286</td>\n",
       "      <td>31.857</td>\n",
       "      <td>59.571</td>\n",
       "      <td>46.952</td>\n",
       "      <td>12.476</td>\n",
       "      <td>65.476</td>\n",
       "      <td>62.000</td>\n",
       "      <td>65.762</td>\n",
       "      <td>23.571</td>\n",
       "      <td>187.333</td>\n",
       "      <td>37.667</td>\n",
       "      <td>...</td>\n",
       "      <td>1615.000</td>\n",
       "      <td>1245.667</td>\n",
       "      <td>1119.667</td>\n",
       "      <td>2325.000</td>\n",
       "      <td>4284.667</td>\n",
       "      <td>3459.667</td>\n",
       "      <td>179.333</td>\n",
       "      <td>5166.000</td>\n",
       "      <td>1590.222</td>\n",
       "      <td>11652.333</td>\n",
       "      <td>699.667</td>\n",
       "      <td>19815.667</td>\n",
       "      <td>640.000</td>\n",
       "      <td>354.333</td>\n",
       "      <td>28150.0</td>\n",
       "      <td>44400.0</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>8350.0</td>\n",
       "      <td>6150.0</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>9020.0</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>3090.0</td>\n",
       "      <td>16490.0</td>\n",
       "      <td>6990.0</td>\n",
       "      <td>11300.0</td>\n",
       "      <td>3040.0</td>\n",
       "      <td>4740.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>34040.0</td>\n",
       "      <td>5120.0</td>\n",
       "      <td>2.55</td>\n",
       "      <td>5.300</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.75</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>20.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>3.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.067</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>4.5</td>\n",
       "      <td>6.567</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-30</th>\n",
       "      <td>7.143</td>\n",
       "      <td>20.381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.333</td>\n",
       "      <td>31.143</td>\n",
       "      <td>22.714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.190</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.286</td>\n",
       "      <td>62.429</td>\n",
       "      <td>4.095</td>\n",
       "      <td>4.381</td>\n",
       "      <td>18.190</td>\n",
       "      <td>62.143</td>\n",
       "      <td>55.571</td>\n",
       "      <td>1.619</td>\n",
       "      <td>0.667</td>\n",
       "      <td>13.381</td>\n",
       "      <td>19.810</td>\n",
       "      <td>30.333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.190</td>\n",
       "      <td>2.857</td>\n",
       "      <td>16.762</td>\n",
       "      <td>19.048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.286</td>\n",
       "      <td>183.143</td>\n",
       "      <td>42.286</td>\n",
       "      <td>94.000</td>\n",
       "      <td>27.286</td>\n",
       "      <td>59.905</td>\n",
       "      <td>62.048</td>\n",
       "      <td>13.190</td>\n",
       "      <td>40.429</td>\n",
       "      <td>31.619</td>\n",
       "      <td>64.810</td>\n",
       "      <td>26.238</td>\n",
       "      <td>137.095</td>\n",
       "      <td>25.095</td>\n",
       "      <td>...</td>\n",
       "      <td>1444.000</td>\n",
       "      <td>1053.667</td>\n",
       "      <td>1067.333</td>\n",
       "      <td>2200.000</td>\n",
       "      <td>4225.667</td>\n",
       "      <td>3464.333</td>\n",
       "      <td>195.000</td>\n",
       "      <td>5036.000</td>\n",
       "      <td>1617.778</td>\n",
       "      <td>12849.333</td>\n",
       "      <td>607.667</td>\n",
       "      <td>21976.000</td>\n",
       "      <td>556.000</td>\n",
       "      <td>305.889</td>\n",
       "      <td>28920.0</td>\n",
       "      <td>46090.0</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>9120.0</td>\n",
       "      <td>6540.0</td>\n",
       "      <td>8740.0</td>\n",
       "      <td>9110.0</td>\n",
       "      <td>4180.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>16060.0</td>\n",
       "      <td>7320.0</td>\n",
       "      <td>12750.0</td>\n",
       "      <td>3110.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2430.0</td>\n",
       "      <td>35810.0</td>\n",
       "      <td>5570.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>5.200</td>\n",
       "      <td>5.4</td>\n",
       "      <td>4.55</td>\n",
       "      <td>15.4</td>\n",
       "      <td>9.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>8.3</td>\n",
       "      <td>3.6</td>\n",
       "      <td>5.8</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.233</td>\n",
       "      <td>3.8</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>6.267</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-30</th>\n",
       "      <td>6.429</td>\n",
       "      <td>12.762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.143</td>\n",
       "      <td>18.714</td>\n",
       "      <td>19.762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000</td>\n",
       "      <td>49.333</td>\n",
       "      <td>18.476</td>\n",
       "      <td>9.619</td>\n",
       "      <td>35.667</td>\n",
       "      <td>34.238</td>\n",
       "      <td>56.381</td>\n",
       "      <td>2.476</td>\n",
       "      <td>1.810</td>\n",
       "      <td>4.524</td>\n",
       "      <td>30.667</td>\n",
       "      <td>37.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.952</td>\n",
       "      <td>5.905</td>\n",
       "      <td>13.429</td>\n",
       "      <td>43.810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>78.048</td>\n",
       "      <td>175.238</td>\n",
       "      <td>39.619</td>\n",
       "      <td>81.667</td>\n",
       "      <td>25.571</td>\n",
       "      <td>63.905</td>\n",
       "      <td>48.381</td>\n",
       "      <td>27.524</td>\n",
       "      <td>35.429</td>\n",
       "      <td>31.905</td>\n",
       "      <td>63.905</td>\n",
       "      <td>21.476</td>\n",
       "      <td>111.762</td>\n",
       "      <td>30.905</td>\n",
       "      <td>...</td>\n",
       "      <td>2545.333</td>\n",
       "      <td>1759.667</td>\n",
       "      <td>1683.000</td>\n",
       "      <td>2685.000</td>\n",
       "      <td>4854.333</td>\n",
       "      <td>3951.333</td>\n",
       "      <td>289.333</td>\n",
       "      <td>5557.333</td>\n",
       "      <td>1927.667</td>\n",
       "      <td>13522.667</td>\n",
       "      <td>705.667</td>\n",
       "      <td>20945.000</td>\n",
       "      <td>959.000</td>\n",
       "      <td>373.000</td>\n",
       "      <td>29200.0</td>\n",
       "      <td>45050.0</td>\n",
       "      <td>2210.0</td>\n",
       "      <td>9370.0</td>\n",
       "      <td>6360.0</td>\n",
       "      <td>8610.0</td>\n",
       "      <td>9010.0</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>3580.0</td>\n",
       "      <td>3530.0</td>\n",
       "      <td>17470.0</td>\n",
       "      <td>7230.0</td>\n",
       "      <td>13290.0</td>\n",
       "      <td>3190.0</td>\n",
       "      <td>5090.0</td>\n",
       "      <td>2920.0</td>\n",
       "      <td>34280.0</td>\n",
       "      <td>5710.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>5.167</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.20</td>\n",
       "      <td>14.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6.133</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.167</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>7.286</td>\n",
       "      <td>12.286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.476</td>\n",
       "      <td>26.000</td>\n",
       "      <td>19.524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.619</td>\n",
       "      <td>46.286</td>\n",
       "      <td>6.476</td>\n",
       "      <td>1.571</td>\n",
       "      <td>28.333</td>\n",
       "      <td>37.905</td>\n",
       "      <td>49.952</td>\n",
       "      <td>5.143</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.619</td>\n",
       "      <td>18.476</td>\n",
       "      <td>15.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.714</td>\n",
       "      <td>19.381</td>\n",
       "      <td>0.000</td>\n",
       "      <td>25.381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.857</td>\n",
       "      <td>175.476</td>\n",
       "      <td>49.619</td>\n",
       "      <td>105.000</td>\n",
       "      <td>57.333</td>\n",
       "      <td>81.000</td>\n",
       "      <td>46.762</td>\n",
       "      <td>23.000</td>\n",
       "      <td>51.810</td>\n",
       "      <td>60.619</td>\n",
       "      <td>63.143</td>\n",
       "      <td>22.048</td>\n",
       "      <td>115.238</td>\n",
       "      <td>35.095</td>\n",
       "      <td>...</td>\n",
       "      <td>1915.667</td>\n",
       "      <td>1342.667</td>\n",
       "      <td>1206.667</td>\n",
       "      <td>2703.333</td>\n",
       "      <td>3709.333</td>\n",
       "      <td>2886.667</td>\n",
       "      <td>183.000</td>\n",
       "      <td>5151.667</td>\n",
       "      <td>1467.222</td>\n",
       "      <td>9602.333</td>\n",
       "      <td>596.000</td>\n",
       "      <td>16656.000</td>\n",
       "      <td>551.667</td>\n",
       "      <td>330.000</td>\n",
       "      <td>30490.0</td>\n",
       "      <td>48280.0</td>\n",
       "      <td>2210.0</td>\n",
       "      <td>9420.0</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>9070.0</td>\n",
       "      <td>9280.0</td>\n",
       "      <td>4230.0</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>3840.0</td>\n",
       "      <td>17250.0</td>\n",
       "      <td>7750.0</td>\n",
       "      <td>13690.0</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>5130.0</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>36680.0</td>\n",
       "      <td>5740.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>4.867</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.05</td>\n",
       "      <td>14.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.6</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5.767</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>4.1</td>\n",
       "      <td>6.100</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 918 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              10                                                                                                                       11                                                                                                                                            112                                                                                                               ...   value                                                                                                                                     gdp                                                                                                                                                unempl                                                                                                      \n",
       "            AT+CH  BE+NL+LU  BG  CZ+SK    ES      FR      GB    GR   HR   HU   IE     IT   LV+LT+EE    PL    PT   RO  SE+FI+DK  SI   AT+CH  BE+NL+LU    BG    CZ+SK     ES      FR      GB      GR     HR      HU      IE      IT   LV+LT+EE    PL      PT      RO   SE+FI+DK  SI   AT+CH  BE+NL+LU    BG    CZ+SK      ES      FR      GB      GR      HR      HU      IE      IT   LV+LT+EE    PL    ...     ES        FR        GB        GR        HR        HU       IE        IT     LV+LT+EE     PL        PT        RO     SE+FI+DK    SI     AT+CH   BE+NL+LU    BG    CZ+SK     ES      FR      GB      GR      HR      HU      IE       IT   LV+LT+EE    PL      PT      RO   SE+FI+DK    SI   AT+CH  BE+NL+LU  BG  CZ+SK   ES   FR   GB    GR    HR   HU   IE    IT  LV+LT+EE  PL   PT   RO  SE+FI+DK  SI \n",
       "date                                                                                                                                                                                                                                                                                                                                                                                                   ...                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "2017-03-31  10.524  16.048   0.0   0.0  26.667  21.857  21.381  0.0  0.0  0.0  0.0  14.095    0.0     6.095  0.0  0.0  21.286   0.0  34.714  60.190    2.190  10.857  38.714  29.810  53.381  10.000  0.000   0.619  11.095  22.048    0.0    11.952  12.714  11.571  26.524   0.0  97.190  178.952  42.238  127.048  33.143  63.333  47.286   9.000  63.095  62.810  59.333  14.905  179.429  39.524  ...  1689.000  1268.333  1080.333  2327.333  4459.000  3747.667  156.000  5255.333  1396.333  12264.667  866.333  18186.333  654.333  420.333  28780.0  42970.0  1510.0  7620.0  5950.0  8450.0  9080.0  3760.0  2590.0  2820.0  14640.0  6800.0  10490.0  2760.0  4520.0  1850.0  33780.0  4790.0  2.85    6.433   6.6  6.00  18.2  9.6  4.6  22.4  12.7  4.3  7.4  11.6   7.333   5.3  9.8  5.3   7.167   7.4\n",
       "2017-06-30   4.952  14.810   0.0   0.0  21.048  13.952  18.857  0.0  0.0  0.0  0.0  15.286    0.0     6.524  0.0  0.0  27.714   0.0  39.286  61.619   10.619   7.714  24.381  27.952  55.714   4.714  2.333   7.095  20.619  17.286    0.0    13.810  23.143  11.143  32.857   0.0  85.619  168.905  35.190   83.429  23.810  57.857  48.000  11.905  33.048  27.905  62.810  14.286  126.667  21.476  ...  1427.667  1023.333  1049.000  2113.333  4121.333  3637.667  164.667  4699.000  1407.000  12940.000  609.667  19264.000  520.333  365.000  29250.0  44540.0  1800.0  8470.0  6330.0  8560.0  8980.0  4120.0  2990.0  3200.0  14700.0  7140.0  11780.0  2960.0  4790.0  2200.0  35590.0  5260.0  2.70    5.867   6.2  5.75  17.3  9.6  4.4  21.8  11.5  4.3  6.7  11.3   7.667   5.1  9.2  4.9   7.133   6.6\n",
       "2017-09-30   4.905  16.095   0.0   0.0  12.429  21.048  16.048  0.0  0.0  0.0  0.0  11.810    0.0    10.762  0.0  0.0   9.667   0.0  35.762  68.952   16.095   8.143  20.476  33.762  56.000   6.762  0.000   9.095  21.333  29.952    0.0    18.048   8.286  11.190  30.619   0.0  89.143  171.190  32.143   75.905  21.810  60.381  42.952  11.429  28.667  28.048  67.143  16.476  145.048  25.571  ...  2495.333  1796.333  1691.667  2704.000  5043.000  4514.000  283.333  5486.000  1742.667  14442.333  743.667  20279.000  960.000  388.000  28610.0  43130.0  2010.0  8810.0  6180.0  8400.0  8650.0  4420.0  3380.0  3360.0  16280.0  7120.0  12240.0  3000.0  4850.0  2650.0  34270.0  5330.0  2.75    5.733   6.1  5.30  16.8  9.5  4.2  21.0  10.3  4.1  6.6  11.3   7.067   4.7  8.7  4.8   7.033   6.6\n",
       "2017-12-31   4.667  18.905   0.0   0.0  15.190  14.143  16.762  0.0  0.0  0.0  0.0  13.952    0.0    10.667  0.0  0.0  22.238   0.0  34.429  42.619    4.000   1.952  17.476  36.667  45.524   1.810  2.190   3.333  23.286  26.667    0.0     9.429  12.381   9.667  22.286   0.0  85.048  146.714  37.381  104.048  29.143  54.810  38.762  10.476  46.333  40.667  59.048  19.429  157.095  27.857  ...  1842.333  1358.667  1138.667  2784.667  3961.333  3252.667  167.000  4942.000  1482.333   9845.667  570.000  15346.333  564.333  348.000  29160.0  46720.0  2070.0  9010.0  6520.0  8850.0  9000.0  4170.0  2950.0  3590.0  16920.0  7640.0  12460.0  3440.0  4860.0  2850.0  36230.0  5440.0  2.70    5.367   5.8  5.10  16.6  9.0  4.2  20.9  10.0  3.9  6.3  10.8   6.767   4.4  8.1  4.7   6.800   5.8\n",
       "2018-03-31   4.429  10.762   0.0   0.0  16.905  18.762  20.429  0.0  0.0  0.0  0.0  13.476    0.0     5.857  0.0  0.0  21.143   0.0  40.000  36.333    8.714   7.952  25.810  25.619  52.286   1.190  0.714   3.857  23.143  33.476    0.0    17.667  34.714   5.429  28.810   0.0  74.000  174.667  48.333  140.286  31.857  59.571  46.952  12.476  65.476  62.000  65.762  23.571  187.333  37.667  ...  1615.000  1245.667  1119.667  2325.000  4284.667  3459.667  179.333  5166.000  1590.222  11652.333  699.667  19815.667  640.000  354.333  28150.0  44400.0  1620.0  8350.0  6150.0  8670.0  9020.0  3830.0  2750.0  3090.0  16490.0  6990.0  11300.0  3040.0  4740.0  1990.0  34040.0  5120.0  2.55    5.300   5.5  4.75  16.2  9.2  4.2  20.3   9.3  3.7  6.0  11.0   7.067   4.0  7.6  4.5   6.567   5.6\n",
       "2018-06-30   7.143  20.381   0.0   0.0  15.333  31.143  22.714  0.0  0.0  0.0  0.0  17.190    0.0    12.143  0.0  0.0  14.810   0.0  36.286  62.429    4.095   4.381  18.190  62.143  55.571   1.619  0.667  13.381  19.810  30.333    0.0    19.190   2.857  16.762  19.048   0.0  73.286  183.143  42.286   94.000  27.286  59.905  62.048  13.190  40.429  31.619  64.810  26.238  137.095  25.095  ...  1444.000  1053.667  1067.333  2200.000  4225.667  3464.333  195.000  5036.000  1617.778  12849.333  607.667  21976.000  556.000  305.889  28920.0  46090.0  1940.0  9120.0  6540.0  8740.0  9110.0  4180.0  3210.0  3440.0  16060.0  7320.0  12750.0  3110.0  5000.0  2430.0  35810.0  5570.0  2.35    5.200   5.4  4.55  15.4  9.2  4.0  19.5   8.3  3.6  5.8  11.0   6.233   3.8  7.1  4.2   6.267   5.3\n",
       "2018-09-30   6.429  12.762   0.0   0.0  28.143  18.714  19.762  0.0  0.0  0.0  0.0  18.476    0.0     6.571  0.0  0.0  17.286   0.0  33.000  49.333   18.476   9.619  35.667  34.238  56.381   2.476  1.810   4.524  30.667  37.810    0.0    15.952   5.905  13.429  43.810   0.0  78.048  175.238  39.619   81.667  25.571  63.905  48.381  27.524  35.429  31.905  63.905  21.476  111.762  30.905  ...  2545.333  1759.667  1683.000  2685.000  4854.333  3951.333  289.333  5557.333  1927.667  13522.667  705.667  20945.000  959.000  373.000  29200.0  45050.0  2210.0  9370.0  6360.0  8610.0  9010.0  4500.0  3580.0  3530.0  17470.0  7230.0  13290.0  3190.0  5090.0  2920.0  34280.0  5710.0  2.45    5.167   5.2  4.20  14.9  8.9  4.0  19.0   8.3  3.8  5.6  10.1   6.133   3.9  6.9  3.9   6.167   5.1\n",
       "2018-12-31   7.286  12.286   0.0   0.0  24.476  26.000  19.524  0.0  0.0  0.0  0.0  21.143    0.0    11.381  0.0  0.0  14.429   0.0  24.619  46.286    6.476   1.571  28.333  37.905  49.952   5.143  0.000   1.619  18.476  15.000    0.0    23.714  19.381   0.000  25.381   0.0  86.857  175.476  49.619  105.000  57.333  81.000  46.762  23.000  51.810  60.619  63.143  22.048  115.238  35.095  ...  1915.667  1342.667  1206.667  2703.333  3709.333  2886.667  183.000  5151.667  1467.222   9602.333  596.000  16656.000  551.667  330.000  30490.0  48280.0  2210.0  9420.0  6730.0  9070.0  9280.0  4230.0  3150.0  3840.0  17250.0  7750.0  13690.0  3620.0  5130.0  3150.0  36680.0  5740.0  2.40    4.867   4.8  4.05  14.6  8.8  3.9  18.4   7.7  3.7  5.6  10.5   5.767   3.8  6.7  4.1   6.100   4.5\n",
       "\n",
       "[8 rows x 918 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of countries\n",
    "countries = eumf_data.get_countries()\n",
    "countries.remove(\"CY\")\n",
    "\n",
    "# migration rates\n",
    "df_values = eumf_data.load_registrations_from_csv(impute_missing=True, countries=countries)\n",
    "\n",
    "# google trends\n",
    "df_trends = eumf_data.load_trends_from_csv(countries=countries)\n",
    "keyword_ids = df_trends.columns.levels[0].tolist()\n",
    "\n",
    "# macroeconomic data\n",
    "df_gdp = eumf_data.read_gdp(countries=countries)\n",
    "df_unempl = eumf_data.read_unempl(countries=countries)\n",
    "\n",
    "country_combinations = [\n",
    "    # [\"GR\", \"CY\"],\n",
    "    [\"LV\", \"LT\", \"EE\"],\n",
    "    [\"BE\", \"NL\", \"LU\"],\n",
    "    [\"CZ\", \"SK\"],\n",
    "    [\"SE\", \"FI\", \"DK\"],\n",
    "    [\"AT\", \"CH\"]\n",
    "]\n",
    "\n",
    "panel = df_values.join(df_trends, how=\"outer\")\n",
    "panel_3m = panel.resample(\"3M\", closed=\"left\").mean()\n",
    "\n",
    "panel_comb = eumf_data.combine_countries(panel, combinations=country_combinations)\n",
    "panel_comb_3m = eumf_data.combine_countries(panel_3m, combinations=country_combinations)\n",
    "df_gdp_comb = eumf_data.combine_countries(df_gdp, combinations=country_combinations)\n",
    "df_unempl_comb = eumf_data.combine_countries(\n",
    "    df_unempl, combinations=country_combinations, average=True\n",
    ")\n",
    "# note: strictly, unweighted average is wrong for unemployment, but should work in most cases\n",
    "\n",
    "panel_comb_3m_macro = panel_comb_3m.join(df_gdp_comb).join(df_unempl_comb)\n",
    "\n",
    "panel_comb_3m_macro[\"2017\":\"2018\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_MIN = \"2010\"\n",
    "T_MAX = \"2019\"\n",
    "\n",
    "T_TEST_MIN = \"2019\"\n",
    "T_TEST_MAX = \"2019\"\n",
    "\n",
    "cv_default = eumf_eval.BlockKFold(n_splits=8, margin=1.0)\n",
    "cv_random = model_selection.KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# n_countries = len(panel_comb_3m_macro[\"value\"].columns)\n",
    "# cv_time = model_selection.TimeSeriesSplit(test_size=4 * n_countries, n_splits=6)\n",
    "\n",
    "cv_scores_global, test_scores_global = [], []\n",
    "\n",
    "with open(\"best_feature_combinations.json\") as f:\n",
    "    best_feature_combinations = json.load(f)\n",
    "\n",
    "selected_keyword_ids = [\n",
    "    \"19\",\n",
    "    \"118\",\n",
    "    \"28\",\n",
    "    \"24\",\n",
    "    \"123\",\n",
    "    \"119\",\n",
    "    \"39\",\n",
    "    \"115\",\n",
    "    \"124\",\n",
    "    \"117\",\n",
    "]\n",
    "\n",
    "lags_default = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "alternate_lags_default = {\n",
    "    \"value\": [2, 3, 4, 5, 6, 7, 8],\n",
    "    \"gdp\": [2, 3, 4, 5, 6, 7, 8],\n",
    "    \"unempl\": [2, 3, 4, 5, 6, 7, 8],\n",
    "}\n",
    "\n",
    "features = [\"value\", \"gdp\", \"unempl\"] + selected_keyword_ids\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 15 is smaller than n_iter=20. Running 15 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "# rough tuning already performed, this is the final search space\n",
    "params = {\n",
    "    \"randomforestregressor__max_features\": [None],\n",
    "    \"randomforestregressor__min_samples_leaf\": [4, 6, 8, 10, 12],\n",
    "    \"randomforestregressor__min_samples_split\": [2, 4, 8],\n",
    "}\n",
    "\n",
    "labeled = eumf_pipeline.prepare_data(\n",
    "    panel_comb_3m_macro,\n",
    "    columns=features,\n",
    "    lags=lags_default,\n",
    "    alternate_lags=alternate_lags_default,\n",
    "    t_min=T_MIN,\n",
    "    t_max=T_MAX,\n",
    ")\n",
    "\n",
    "labeled.x = labeled.x[best_feature_combinations[\"ensemble\"][\"all\"]]\n",
    "transformed = eumf_pipeline.transform_data(labeled)\n",
    "train, test = eumf_pipeline.split_data(\n",
    "    transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    ")\n",
    "train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "\n",
    "tuner = eumf_pipeline.train_reg_model(\n",
    "    train_stacked,\n",
    "    reg=ensemble.RandomForestRegressor(random_state=42),\n",
    "    extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "    params=params,\n",
    "    scoring=eumf_eval.scorer_mae,\n",
    "    cv=cv_default,\n",
    "    random_iterations=20,\n",
    ")\n",
    "\n",
    "cv_score = eumf_eval.score_cv(\n",
    "    tuner.best_estimator_,\n",
    "    train_stacked,\n",
    "    cv=cv_default,\n",
    ")\n",
    "\n",
    "test_score = eumf_eval.score_test(\n",
    "    tuner.best_estimator_,\n",
    "    test_stacked,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_randomforestregressor__min_samples_split</th>\n",
       "      <th>param_randomforestregressor__min_samples_leaf</th>\n",
       "      <th>param_randomforestregressor__max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.219</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 2...</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.765</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.016</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 4...</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.282</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.007</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 8...</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.529</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 2...</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.717</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.019</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 4...</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.746</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 8...</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.541</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 2...</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.792</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 4...</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.202</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.014</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 8...</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.245</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 2...</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.025</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.617</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.010</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 4...</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.025</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.410</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.004</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 8...</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.025</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.783</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.026</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 2...</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.026</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.567</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.016</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 4...</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.026</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.096</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.005</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>{'randomforestregressor__min_samples_split': 8...</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.026</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_randomforestregressor__min_samples_split param_randomforestregressor__min_samples_leaf param_randomforestregressor__max_features                       params                        split0_test_score  split1_test_score  split2_test_score  split3_test_score  split4_test_score  split5_test_score  split6_test_score  split7_test_score  mean_test_score  std_test_score  rank_test_score\n",
       "9       2.219          0.531          0.034            0.009                            2                                             10                                         None                    {'randomforestregressor__min_samples_split': 2...       -0.115             -0.093             -0.088             -0.111             -0.060             -0.084             -0.059             -0.047            -0.082            0.023             1       \n",
       "10      2.765          0.275          0.037            0.016                            4                                             10                                         None                    {'randomforestregressor__min_samples_split': 4...       -0.115             -0.093             -0.088             -0.111             -0.060             -0.084             -0.059             -0.047            -0.082            0.023             1       \n",
       "11      2.282          0.187          0.036            0.007                            8                                             10                                         None                    {'randomforestregressor__min_samples_split': 8...       -0.115             -0.093             -0.088             -0.111             -0.060             -0.084             -0.059             -0.047            -0.082            0.023             1       \n",
       "6       2.529          0.385          0.038            0.010                            2                                              8                                         None                    {'randomforestregressor__min_samples_split': 2...       -0.117             -0.097             -0.088             -0.109             -0.059             -0.084             -0.059             -0.046            -0.083            0.024             4       \n",
       "7       2.717          0.594          0.044            0.019                            4                                              8                                         None                    {'randomforestregressor__min_samples_split': 4...       -0.117             -0.097             -0.088             -0.109             -0.059             -0.084             -0.059             -0.046            -0.083            0.024             4       \n",
       "8       2.746          0.397          0.035            0.009                            8                                              8                                         None                    {'randomforestregressor__min_samples_split': 8...       -0.117             -0.097             -0.088             -0.109             -0.059             -0.084             -0.059             -0.046            -0.083            0.024             4       \n",
       "12      2.541          0.281          0.035            0.012                            2                                             12                                         None                    {'randomforestregressor__min_samples_split': 2...       -0.120             -0.092             -0.089             -0.111             -0.060             -0.083             -0.059             -0.047            -0.083            0.024             7       \n",
       "13      2.792          0.450          0.054            0.020                            4                                             12                                         None                    {'randomforestregressor__min_samples_split': 4...       -0.120             -0.092             -0.089             -0.111             -0.060             -0.083             -0.059             -0.047            -0.083            0.024             7       \n",
       "14      2.202          0.189          0.030            0.014                            8                                             12                                         None                    {'randomforestregressor__min_samples_split': 8...       -0.120             -0.092             -0.089             -0.111             -0.060             -0.083             -0.059             -0.047            -0.083            0.024             7       \n",
       "3       2.245          0.146          0.028            0.003                            2                                              6                                         None                    {'randomforestregressor__min_samples_split': 2...       -0.123             -0.095             -0.088             -0.107             -0.059             -0.086             -0.060             -0.046            -0.083            0.025            10       \n",
       "4       2.617          0.261          0.036            0.010                            4                                              6                                         None                    {'randomforestregressor__min_samples_split': 4...       -0.123             -0.095             -0.088             -0.107             -0.059             -0.086             -0.060             -0.046            -0.083            0.025            10       \n",
       "5       2.410          0.179          0.026            0.004                            8                                              6                                         None                    {'randomforestregressor__min_samples_split': 8...       -0.123             -0.095             -0.088             -0.107             -0.059             -0.086             -0.060             -0.046            -0.083            0.025            10       \n",
       "0       3.783          0.369          0.069            0.026                            2                                              4                                         None                    {'randomforestregressor__min_samples_split': 2...       -0.132             -0.092             -0.088             -0.105             -0.062             -0.083             -0.062             -0.045            -0.084            0.026            13       \n",
       "1       3.567          0.601          0.047            0.016                            4                                              4                                         None                    {'randomforestregressor__min_samples_split': 4...       -0.132             -0.092             -0.088             -0.105             -0.062             -0.083             -0.062             -0.045            -0.084            0.026            13       \n",
       "2       3.096          0.216          0.028            0.005                            8                                              4                                         None                    {'randomforestregressor__min_samples_split': 8...       -0.132             -0.092             -0.088             -0.105             -0.062             -0.083             -0.062             -0.045            -0.084            0.026            13       "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tuner.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "\n",
    "# rough tuning already performed, this is the final search space\n",
    "params = {\n",
    "    \"xgbregressor__eta\": np.linspace(0.05, 0.3, 50),\n",
    "    \"xgbregressor__max_depth\": [6, 7, 8, 9, 10],\n",
    "    \"xgbregressor__min_child_weight\": [2, 4],\n",
    "    \"xgbregressor__max_delta_step\": [2, 3, 4, 5, 6, 7, 8],\n",
    "    \"xgbregressor__subsample\":  np.linspace(0.6, 0.75, 50),\n",
    "    \"xgbregressor__colsample_bytree\": np.linspace(0.7, 0.9, 50),\n",
    "    \"xgbregressor__colsample_bylevel\": np.linspace(0.8, 1.0, 50),\n",
    "    \"xgbregressor__colsample_bynode\": np.linspace(0.6, 1.0, 50),\n",
    "    \"xgbregressor__lambda\": np.linspace(0.8, 0.95, 50),\n",
    "    \"xgbregressor__alpha\": np.linspace(0.75,0.95),\n",
    "}\n",
    "\n",
    "\n",
    "labeled = eumf_pipeline.prepare_data(\n",
    "    panel_comb_3m_macro,\n",
    "    columns=features,\n",
    "    lags=lags_default,\n",
    "    alternate_lags=alternate_lags_default,\n",
    "    t_min=T_MIN,\n",
    "    t_max=T_MAX,\n",
    ")\n",
    "\n",
    "labeled.x = labeled.x[best_feature_combinations[\"ensemble\"][\"all\"]]\n",
    "transformed = eumf_pipeline.transform_data(labeled)\n",
    "train, test = eumf_pipeline.split_data(\n",
    "    transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    ")\n",
    "train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "\n",
    "tuner = eumf_pipeline.train_reg_model(\n",
    "    train_stacked,\n",
    "    reg=xgb.XGBRegressor(random_state=42),\n",
    "    extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "    params=params,\n",
    "    scoring=eumf_eval.scorer_mae,\n",
    "    cv=cv_default,\n",
    "    random_iterations=20,\n",
    ")\n",
    "\n",
    "cv_score = eumf_eval.score_cv(\n",
    "    tuner.best_estimator_,\n",
    "    train_stacked,\n",
    "    cv=cv_default,\n",
    ")\n",
    "\n",
    "test_score = eumf_eval.score_test(\n",
    "    tuner.best_estimator_,\n",
    "    test_stacked,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_xgbregressor__subsample</th>\n",
       "      <th>param_xgbregressor__min_child_weight</th>\n",
       "      <th>param_xgbregressor__max_depth</th>\n",
       "      <th>param_xgbregressor__max_delta_step</th>\n",
       "      <th>param_xgbregressor__lambda</th>\n",
       "      <th>param_xgbregressor__eta</th>\n",
       "      <th>param_xgbregressor__colsample_bytree</th>\n",
       "      <th>param_xgbregressor__colsample_bynode</th>\n",
       "      <th>param_xgbregressor__colsample_bylevel</th>\n",
       "      <th>param_xgbregressor__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.040</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.661</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.864</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6612244897959183...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.085</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.841</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.686</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.819</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6857142857142857...</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.025</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.574</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.624</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.877</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6244897959183673...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.025</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.694</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.744</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.783</td>\n",
       "      <td>{'xgbregressor__subsample': 0.7438775510204081...</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.026</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.664</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.665</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.889</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6642857142857143...</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.023</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.645</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.695</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.881</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6948979591836735...</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.634</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.868</td>\n",
       "      <td>{'xgbregressor__subsample': 0.633673469387755,...</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.026</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.299</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.738</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.872</td>\n",
       "      <td>{'xgbregressor__subsample': 0.7377551020408163...</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>0.023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.627</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.713</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.856</td>\n",
       "      <td>{'xgbregressor__subsample': 0.713265306122449,...</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.027</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.245</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.646</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.828</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6459183673469387...</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.025</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.963</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.683</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.822</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.868</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6826530612244898...</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.025</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.652</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.93</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6520408163265305...</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.025</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.754</td>\n",
       "      <td>{'xgbregressor__subsample': 0.710204081632653,...</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.023</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.812</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.612</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.787</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6122448979591837...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.020</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.603</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.917</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6030612244897959...</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.024</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.549</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.652</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.783</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6520408163265305...</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.028</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.597</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.713</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.913</td>\n",
       "      <td>{'xgbregressor__subsample': 0.713265306122449,...</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.026</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.661</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.926</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6612244897959183...</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.025</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.642</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.68</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.938</td>\n",
       "      <td>{'xgbregressor__subsample': 0.6795918367346938...</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.024</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.512</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.726</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.901</td>\n",
       "      <td>{'xgbregressor__subsample': 0.7255102040816326...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.029</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_xgbregressor__subsample param_xgbregressor__min_child_weight param_xgbregressor__max_depth param_xgbregressor__max_delta_step param_xgbregressor__lambda param_xgbregressor__eta param_xgbregressor__colsample_bytree param_xgbregressor__colsample_bynode param_xgbregressor__colsample_bylevel param_xgbregressor__alpha                       params                        split0_test_score  split1_test_score  split2_test_score  split3_test_score  split4_test_score  split5_test_score  split6_test_score  split7_test_score  mean_test_score  std_test_score  rank_test_score\n",
       "8       1.040          0.079          0.022            0.004                  0.661                              4                                 9                               8                             0.84                     0.076                         0.737                                0.902                                 0.841                           0.864           {'xgbregressor__subsample': 0.6612244897959183...       -0.125             -0.105             -0.095             -0.103             -0.056             -0.080             -0.065             -0.052            -0.085            0.024             1       \n",
       "9       0.841          0.122          0.022            0.008                  0.686                              4                                 6                               8                            0.831                     0.081                         0.798                                0.665                                  0.98                           0.819           {'xgbregressor__subsample': 0.6857142857142857...       -0.128             -0.106             -0.095             -0.103             -0.056             -0.082             -0.064             -0.053            -0.086            0.025             2       \n",
       "4       0.574          0.062          0.017            0.008                  0.624                              4                                 7                               7                            0.864                     0.065                         0.708                                0.641                                 0.869                           0.877           {'xgbregressor__subsample': 0.6244897959183673...       -0.125             -0.102             -0.097             -0.108             -0.054             -0.082             -0.069             -0.052            -0.086            0.025             3       \n",
       "15      0.694          0.066          0.013            0.002                  0.744                              4                                 9                               7                            0.907                     0.172                         0.704                                0.698                                 0.951                           0.783           {'xgbregressor__subsample': 0.7438775510204081...       -0.136             -0.092             -0.106             -0.101             -0.062             -0.075             -0.064             -0.054            -0.086            0.026             4       \n",
       "5       0.478          0.039          0.013            0.002                  0.664                              4                                 9                               8                             0.91                     0.244                         0.712                                0.665                                 0.996                           0.889           {'xgbregressor__subsample': 0.6642857142857143...       -0.119             -0.097             -0.103             -0.113             -0.059             -0.077             -0.069             -0.056            -0.087            0.023             5       \n",
       "17      0.645          0.132          0.019            0.006                  0.695                              4                                10                               7                             0.84                     0.121                         0.745                                0.633                                 0.918                           0.881           {'xgbregressor__subsample': 0.6948979591836735...       -0.129             -0.096             -0.098             -0.110             -0.060             -0.078             -0.068             -0.054            -0.087            0.024             6       \n",
       "0       0.699          0.149          0.022            0.002                  0.634                              2                                 6                               2                             0.91                     0.137                         0.712                                0.714                                 0.922                           0.868           {'xgbregressor__subsample': 0.633673469387755,...       -0.132             -0.110             -0.098             -0.105             -0.059             -0.077             -0.061             -0.055            -0.087            0.026             7       \n",
       "12      1.299          0.169          0.021            0.005                  0.738                              4                                 7                               6                            0.889                     0.116                         0.835                                0.976                                 0.988                           0.872           {'xgbregressor__subsample': 0.7377551020408163...       -0.128             -0.106             -0.096             -0.101             -0.062             -0.081             -0.070             -0.055            -0.087            0.023             8       \n",
       "14      0.627          0.070          0.022            0.010                  0.713                              4                                 7                               8                             0.84                     0.076                           0.9                                0.657                                 0.886                           0.856           {'xgbregressor__subsample': 0.713265306122449,...       -0.129             -0.105             -0.096             -0.115             -0.057             -0.084             -0.066             -0.050            -0.088            0.027             9       \n",
       "7       1.245          0.254          0.029            0.008                  0.646                              2                                 7                               8                            0.864                     0.157                         0.896                                0.984                                 0.812                           0.828           {'xgbregressor__subsample': 0.6459183673469387...       -0.131             -0.098             -0.106             -0.108             -0.064             -0.077             -0.071             -0.050            -0.088            0.025            10       \n",
       "1       0.963          0.092          0.018            0.006                  0.683                              2                                10                               5                            0.907                     0.152                         0.822                                  1.0                                 0.833                           0.868           {'xgbregressor__subsample': 0.6826530612244898...       -0.131             -0.110             -0.101             -0.098             -0.060             -0.076             -0.072             -0.057            -0.088            0.025            11       \n",
       "16      0.495          0.020          0.012            0.001                  0.652                              4                                 7                               6                            0.846                     0.096                         0.835                                  0.6                                  0.89                            0.93           {'xgbregressor__subsample': 0.6520408163265305...       -0.128             -0.099             -0.103             -0.114             -0.059             -0.077             -0.068             -0.057            -0.088            0.025            12       \n",
       "13      0.797          0.096          0.014            0.003                   0.71                              2                                 9                               4                            0.873                     0.147                         0.773                                0.755                                  0.91                           0.754           {'xgbregressor__subsample': 0.710204081632653,...       -0.127             -0.113             -0.087             -0.107             -0.061             -0.079             -0.076             -0.059            -0.089            0.023            13       \n",
       "18      0.812          0.100          0.027            0.012                  0.612                              2                                10                               5                            0.873                     0.239                         0.806                                0.796                                 0.804                           0.787           {'xgbregressor__subsample': 0.6122448979591837...       -0.125             -0.097             -0.106             -0.099             -0.069             -0.082             -0.074             -0.063            -0.090            0.020            14       \n",
       "11      0.714          0.094          0.018            0.005                  0.603                              4                                 7                               5                            0.849                     0.193                          0.72                                0.976                                 0.829                           0.917           {'xgbregressor__subsample': 0.6030612244897959...       -0.125             -0.113             -0.099             -0.112             -0.061             -0.084             -0.065             -0.062            -0.090            0.024            15       \n",
       "2       0.549          0.050          0.023            0.008                  0.652                              2                                 6                               2                            0.843                     0.208                           0.7                                0.714                                 0.898                           0.783           {'xgbregressor__subsample': 0.6520408163265305...       -0.136             -0.109             -0.112             -0.111             -0.057             -0.073             -0.069             -0.055            -0.090            0.028            16       \n",
       "10      0.597          0.113          0.017            0.004                  0.713                              2                                 6                               4                            0.834                      0.29                         0.827                                0.624                                 0.947                           0.913           {'xgbregressor__subsample': 0.713265306122449,...       -0.141             -0.106             -0.100             -0.102             -0.061             -0.081             -0.074             -0.060            -0.091            0.026            17       \n",
       "3       0.606          0.100          0.015            0.003                  0.661                              4                                 9                               4                            0.831                      0.29                         0.761                                0.649                                 0.927                           0.926           {'xgbregressor__subsample': 0.6612244897959183...       -0.126             -0.114             -0.100             -0.114             -0.069             -0.084             -0.065             -0.056            -0.091            0.025            18       \n",
       "19      0.642          0.083          0.015            0.002                   0.68                              4                                10                               3                            0.815                     0.295                         0.814                                 0.69                                 0.845                           0.938           {'xgbregressor__subsample': 0.6795918367346938...       -0.137             -0.107             -0.103             -0.106             -0.062             -0.088             -0.066             -0.069            -0.092            0.024            19       \n",
       "6       0.512          0.126          0.017            0.006                  0.726                              4                                10                               7                            0.828                      0.28                         0.782                                0.731                                 0.849                           0.901           {'xgbregressor__subsample': 0.7255102040816326...       -0.148             -0.105             -0.107             -0.114             -0.066             -0.080             -0.068             -0.057            -0.093            0.029            20       "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tuner.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperparams = pd.DataFrame(tuner.cv_results_).sort_values(\"rank_test_score\")\n",
    "hyperparams.plot.scatter(\"param_xgbregressor__subsample\", \"mean_test_score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "\n",
    "# rough tuning already performed, this is the final search space\n",
    "params = {\n",
    "    \"adaboostregressor__learning_rate\": np.linspace(0.5, 1.5, 11),\n",
    "    \"adaboostregressor__loss\": [\"linear\", \"square\", \"exponential\"],\n",
    "}\n",
    "\n",
    "\n",
    "labeled = eumf_pipeline.prepare_data(\n",
    "    panel_comb_3m_macro,\n",
    "    columns=features,\n",
    "    lags=lags_default,\n",
    "    alternate_lags=alternate_lags_default,\n",
    "    t_min=T_MIN,\n",
    "    t_max=T_MAX,\n",
    ")\n",
    "\n",
    "labeled.x = labeled.x[best_feature_combinations[\"ensemble\"][\"all\"]]\n",
    "transformed = eumf_pipeline.transform_data(labeled)\n",
    "train, test = eumf_pipeline.split_data(\n",
    "    transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    ")\n",
    "train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "\n",
    "tuner = eumf_pipeline.train_reg_model(\n",
    "    train_stacked,\n",
    "    reg=ensemble.AdaBoostRegressor(random_state=42, n_estimators=100),\n",
    "    extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "    params=params,\n",
    "    scoring=eumf_eval.scorer_mae,\n",
    "    cv=cv_default,\n",
    ")\n",
    "\n",
    "cv_score = eumf_eval.score_cv(\n",
    "    tuner.best_estimator_,\n",
    "    train_stacked,\n",
    "    cv=cv_default,\n",
    ")\n",
    "\n",
    "test_score = eumf_eval.score_test(\n",
    "    tuner.best_estimator_,\n",
    "    test_stacked,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_adaboostregressor__learning_rate</th>\n",
       "      <th>param_adaboostregressor__loss</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.088</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.6</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.6, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.084</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.5, 'ada...</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.771</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.5, 'ada...</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.030</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.933</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.7</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.7, 'ada...</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.031</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.635</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.2</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.2000000...</td>\n",
       "      <td>-0.141</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.028</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.197</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.6</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.6, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.030</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.384</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.1</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.1, 'ada...</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.028</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.363</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.5</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.5, 'ada...</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.028</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.047</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.1</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.1, 'ada...</td>\n",
       "      <td>-0.151</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.030</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.708</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.8</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.8, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>0.031</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.535</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.3</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.3, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.030</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.625</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.5, 'ada...</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.028</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.518</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.0, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.029</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.282</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.4</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.4, 'ada...</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.029</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.315</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.7</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.7, 'ada...</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.030</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.607</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.2</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.2000000...</td>\n",
       "      <td>-0.146</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.028</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.098</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.4</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.4, 'ada...</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.032</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.983</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.8</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.8, 'ada...</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.029</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.976</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.7</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.7, 'ada...</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.031</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.321</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.0, 'ada...</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.032</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.470</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.9</td>\n",
       "      <td>exponential</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.9, 'ada...</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.030</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.341</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.3</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.3, 'ada...</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.032</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.466</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.9</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.9, 'ada...</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.031</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.519</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.9, 'ada...</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.124</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.030</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.150</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.025</td>\n",
       "      <td>1.1</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.1, 'ada...</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.032</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.352</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.0, 'ada...</td>\n",
       "      <td>-0.154</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.031</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.008</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.5</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.5, 'ada...</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.040</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.382</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.4</td>\n",
       "      <td>linear</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.4, 'ada...</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.030</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.759</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.6</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.6, 'ada...</td>\n",
       "      <td>-0.161</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>-0.117</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.033</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.227</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.3</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.3, 'ada...</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>0.030</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.739</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.5, 'ada...</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.033</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.587</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.8</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 0.8, 'ada...</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.060</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.031</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.269</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.2</td>\n",
       "      <td>square</td>\n",
       "      <td>{'adaboostregressor__learning_rate': 1.2000000...</td>\n",
       "      <td>-0.156</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.031</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_adaboostregressor__learning_rate param_adaboostregressor__loss                       params                        split0_test_score  split1_test_score  split2_test_score  split3_test_score  split4_test_score  split5_test_score  split6_test_score  split7_test_score  mean_test_score  std_test_score  rank_test_score\n",
       "5       2.088          0.364          0.104            0.060                       0.6                            exponential          {'adaboostregressor__learning_rate': 0.6, 'ada...       -0.148             -0.094             -0.097             -0.119             -0.061             -0.087             -0.062             -0.052            -0.090            0.030             1       \n",
       "0       2.084          0.389          0.145            0.041                       0.5                                 linear          {'adaboostregressor__learning_rate': 0.5, 'ada...       -0.149             -0.092             -0.098             -0.116             -0.063             -0.088             -0.066             -0.052            -0.091            0.030             2       \n",
       "2       1.771          0.408          0.099            0.063                       0.5                            exponential          {'adaboostregressor__learning_rate': 0.5, 'ada...       -0.149             -0.094             -0.102             -0.117             -0.061             -0.087             -0.066             -0.051            -0.091            0.030             3       \n",
       "8       1.933          0.336          0.111            0.037                       0.7                            exponential          {'adaboostregressor__learning_rate': 0.7, 'ada...       -0.150             -0.094             -0.099             -0.121             -0.062             -0.087             -0.061             -0.055            -0.091            0.031             4       \n",
       "23      1.635          0.232          0.066            0.014                       1.2                            exponential          {'adaboostregressor__learning_rate': 1.2000000...       -0.141             -0.097             -0.097             -0.122             -0.060             -0.090             -0.064             -0.059            -0.091            0.028             5       \n",
       "3       2.197          0.247          0.106            0.085                       0.6                                 linear          {'adaboostregressor__learning_rate': 0.6, 'ada...       -0.148             -0.094             -0.100             -0.118             -0.060             -0.090             -0.068             -0.053            -0.091            0.030             6       \n",
       "18      1.384          0.140          0.062            0.009                       1.1                                 linear          {'adaboostregressor__learning_rate': 1.1, 'ada...       -0.147             -0.095             -0.105             -0.112             -0.060             -0.089             -0.066             -0.058            -0.091            0.028             7       \n",
       "30      1.363          0.132          0.062            0.010                       1.5                                 linear          {'adaboostregressor__learning_rate': 1.5, 'ada...       -0.144             -0.095             -0.102             -0.117             -0.060             -0.089             -0.067             -0.058            -0.091            0.028             8       \n",
       "20      2.047          0.252          0.082            0.023                       1.1                            exponential          {'adaboostregressor__learning_rate': 1.1, 'ada...       -0.151             -0.093             -0.100             -0.117             -0.061             -0.090             -0.066             -0.055            -0.091            0.030             9       \n",
       "11      1.708          0.163          0.085            0.034                       0.8                            exponential          {'adaboostregressor__learning_rate': 0.8, 'ada...       -0.148             -0.092             -0.104             -0.121             -0.059             -0.091             -0.063             -0.053            -0.091            0.031            10       \n",
       "26      1.535          0.123          0.079            0.016                       1.3                            exponential          {'adaboostregressor__learning_rate': 1.3, 'ada...       -0.148             -0.093             -0.101             -0.123             -0.060             -0.090             -0.065             -0.054            -0.092            0.030            11       \n",
       "32      1.625          0.143          0.047            0.014                       1.5                            exponential          {'adaboostregressor__learning_rate': 1.5, 'ada...       -0.146             -0.091             -0.099             -0.119             -0.064             -0.090             -0.067             -0.058            -0.092            0.028            12       \n",
       "17      1.518          0.139          0.067            0.030                       1.0                            exponential          {'adaboostregressor__learning_rate': 1.0, 'ada...       -0.148             -0.095             -0.098             -0.120             -0.061             -0.090             -0.070             -0.055            -0.092            0.029            13       \n",
       "29      1.282          0.094          0.073            0.023                       1.4                            exponential          {'adaboostregressor__learning_rate': 1.4, 'ada...       -0.146             -0.094             -0.100             -0.122             -0.061             -0.088             -0.066             -0.059            -0.092            0.029            14       \n",
       "7       1.315          0.093          0.084            0.028                       0.7                                 square          {'adaboostregressor__learning_rate': 0.7, 'ada...       -0.148             -0.097             -0.104             -0.117             -0.061             -0.093             -0.065             -0.054            -0.092            0.030            15       \n",
       "21      1.607          0.144          0.075            0.020                       1.2                                 linear          {'adaboostregressor__learning_rate': 1.2000000...       -0.146             -0.093             -0.100             -0.119             -0.062             -0.090             -0.067             -0.064            -0.093            0.028            16       \n",
       "28      1.098          0.054          0.065            0.011                       1.4                                 square          {'adaboostregressor__learning_rate': 1.4, 'ada...       -0.158             -0.101             -0.096             -0.111             -0.058             -0.095             -0.071             -0.051            -0.093            0.032            17       \n",
       "9       1.983          0.226          0.085            0.027                       0.8                                 linear          {'adaboostregressor__learning_rate': 0.8, 'ada...       -0.147             -0.095             -0.104             -0.120             -0.062             -0.088             -0.068             -0.057            -0.093            0.029            18       \n",
       "6       1.976          0.346          0.069            0.018                       0.7                                 linear          {'adaboostregressor__learning_rate': 0.7, 'ada...       -0.155             -0.092             -0.101             -0.119             -0.059             -0.089             -0.069             -0.058            -0.093            0.031            19       \n",
       "16      1.321          0.251          0.075            0.031                       1.0                                 square          {'adaboostregressor__learning_rate': 1.0, 'ada...       -0.156             -0.101             -0.102             -0.116             -0.057             -0.093             -0.067             -0.051            -0.093            0.032            20       \n",
       "14      1.470          0.136          0.059            0.011                       0.9                            exponential          {'adaboostregressor__learning_rate': 0.9, 'ada...       -0.150             -0.097             -0.100             -0.125             -0.062             -0.087             -0.065             -0.059            -0.093            0.030            21       \n",
       "24      1.341          0.160          0.074            0.029                       1.3                                 linear          {'adaboostregressor__learning_rate': 1.3, 'ada...       -0.156             -0.094             -0.104             -0.118             -0.059             -0.092             -0.068             -0.055            -0.093            0.032            22       \n",
       "13      1.466          0.141          0.067            0.018                       0.9                                 square          {'adaboostregressor__learning_rate': 0.9, 'ada...       -0.156             -0.096             -0.103             -0.116             -0.058             -0.094             -0.071             -0.054            -0.093            0.031            23       \n",
       "12      1.519          0.150          0.065            0.010                       0.9                                 linear          {'adaboostregressor__learning_rate': 0.9, 'ada...       -0.149             -0.099             -0.102             -0.124             -0.059             -0.088             -0.070             -0.056            -0.093            0.030            24       \n",
       "19      1.150          0.081          0.077            0.025                       1.1                                 square          {'adaboostregressor__learning_rate': 1.1, 'ada...       -0.162             -0.095             -0.098             -0.109             -0.059             -0.099             -0.068             -0.057            -0.093            0.032            25       \n",
       "15      1.352          0.129          0.063            0.009                       1.0                                 linear          {'adaboostregressor__learning_rate': 1.0, 'ada...       -0.154             -0.091             -0.103             -0.120             -0.061             -0.091             -0.069             -0.057            -0.093            0.031            26       \n",
       "31      1.008          0.129          0.078            0.017                       1.5                                 square          {'adaboostregressor__learning_rate': 1.5, 'ada...       -0.189             -0.095             -0.096             -0.100             -0.057             -0.094             -0.070             -0.052            -0.094            0.040            27       \n",
       "27      1.382          0.076          0.066            0.012                       1.4                                 linear          {'adaboostregressor__learning_rate': 1.4, 'ada...       -0.155             -0.092             -0.103             -0.120             -0.061             -0.092             -0.069             -0.061            -0.094            0.030            28       \n",
       "4       1.759          0.207          0.086            0.016                       0.6                                 square          {'adaboostregressor__learning_rate': 0.6, 'ada...       -0.161             -0.099             -0.104             -0.117             -0.057             -0.091             -0.068             -0.058            -0.094            0.033            29       \n",
       "25      1.227          0.110          0.066            0.011                       1.3                                 square          {'adaboostregressor__learning_rate': 1.3, 'ada...       -0.160             -0.097             -0.094             -0.111             -0.062             -0.097             -0.070             -0.066            -0.094            0.030            30       \n",
       "1       1.739          0.102          0.067            0.009                       0.5                                 square          {'adaboostregressor__learning_rate': 0.5, 'ada...       -0.159             -0.093             -0.106             -0.122             -0.061             -0.099             -0.064             -0.054            -0.095            0.033            31       \n",
       "10      1.587          0.164          0.081            0.016                       0.8                                 square          {'adaboostregressor__learning_rate': 0.8, 'ada...       -0.157             -0.099             -0.103             -0.116             -0.060             -0.094             -0.067             -0.061            -0.095            0.031            32       \n",
       "22      1.269          0.102          0.073            0.014                       1.2                                 square          {'adaboostregressor__learning_rate': 1.2000000...       -0.156             -0.103             -0.102             -0.116             -0.059             -0.095             -0.072             -0.058            -0.095            0.031            33       "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tuner.cv_results_).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include country variables (aka \"fixed effects\") in random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:09<00:00, 34.80s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "fixed_effects = [True, False]\n",
    "model_names = [\"rf_fe\", \"rf_no_fe\"]\n",
    "\n",
    "models = [\n",
    "    ensemble.RandomForestRegressor(random_state=42),\n",
    "    ensemble.RandomForestRegressor(random_state=42),\n",
    "]\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=models[i],\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=preprocessing.OneHotEncoder() if fixed_effects else \"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rf_fe</th>\n",
       "      <td>2.501</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.457e-04</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_no_fe</th>\n",
       "      <td>2.565</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.457e-04</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fit_time               score_time               test_mae               train_mae                   test_rmse              train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "           mean    std    sem      mean     std    sem     mean    std    sem      mean    std      sem        mean    std   sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "rf_fe      2.501   0.217  0.077    0.016    0.003  0.001  -0.088   0.031  0.011   -0.028   0.002  6.457e-04   -0.126   0.05  0.018   -0.044    0.004  0.001           0.292          0.189  0.067           0.944           0.004  0.001     0.395    0.237  0.084     0.951     0.005  0.002      0.039      0.044  0.016       0.101      0.016  0.006\n",
       "rf_no_fe   2.565   0.343  0.121    0.017    0.003  0.001  -0.088   0.031  0.011   -0.028   0.002  6.457e-04   -0.126   0.05  0.018   -0.044    0.004  0.001           0.292          0.189  0.067           0.944           0.004  0.001     0.395    0.237  0.084     0.951     0.005  0.002      0.039      0.044  0.016       0.101      0.016  0.006"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:37<00:00,  5.34s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "model_names = [\n",
    "    \"linreg\",\n",
    "    \"elasticnet\",\n",
    "    \"sgd\",\n",
    "    \"ard\",\n",
    "    \"bayesian\",\n",
    "    \"huber\",\n",
    "    \"theil\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "    linear_model.LinearRegression(),\n",
    "    linear_model.ElasticNetCV(),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.ARDRegression(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.HuberRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "]\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[best_feature_combinations[\"linear\"][\"all\"]]\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=models[i],\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=\"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linreg</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.297e-02</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.017</td>\n",
       "      <td>5.945e-03</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elasticnet</th>\n",
       "      <td>0.373</td>\n",
       "      <td>0.078</td>\n",
       "      <td>2.766e-02</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.263e-03</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sgd</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.336e-04</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.349e-04</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ard</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.015</td>\n",
       "      <td>5.237e-03</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.023e-04</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bayesian</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.008</td>\n",
       "      <td>2.835e-03</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.185e-03</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huber</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.944e-03</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.665e-04</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>theil</th>\n",
       "      <td>1.959</td>\n",
       "      <td>0.189</td>\n",
       "      <td>6.679e-02</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.878e-04</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fit_time                   score_time                   test_mae               train_mae               test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "             mean    std      sem        mean     std      sem       mean    std    sem      mean    std    sem      mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "linreg       0.047   0.037  1.297e-02    0.025    0.017  5.945e-03  -0.072   0.018  0.006   -0.069   0.004  0.001   -0.110   0.039  0.014   -0.104    0.009  0.003           0.371          0.224  0.079           0.688           0.034  0.012     0.498    0.254  0.090     0.730     0.045  0.016      0.054      0.056  0.020       0.060      0.014  0.005\n",
       "elasticnet   0.373   0.078  2.766e-02    0.019    0.009  3.263e-03  -0.072   0.018  0.006   -0.069   0.004  0.001   -0.110   0.039  0.014   -0.104    0.009  0.003           0.377          0.216  0.076           0.688           0.035  0.012     0.502    0.245  0.087     0.729     0.045  0.016      0.055      0.056  0.020       0.060      0.014  0.005\n",
       "sgd          0.011   0.002  7.336e-04    0.009    0.001  4.349e-04  -0.076   0.020  0.007   -0.072   0.004  0.002   -0.114   0.041  0.015   -0.107    0.009  0.003           0.350          0.206  0.073           0.667           0.035  0.012     0.475    0.253  0.089     0.711     0.046  0.016      0.051      0.056  0.020       0.057      0.014  0.005\n",
       "ard          0.038   0.015  5.237e-03    0.010    0.002  6.023e-04  -0.074   0.018  0.006   -0.070   0.004  0.001   -0.111   0.041  0.014   -0.106    0.009  0.003           0.383          0.200  0.071           0.676           0.038  0.013     0.498    0.234  0.083     0.719     0.049  0.017      0.053      0.056  0.020       0.059      0.015  0.005\n",
       "bayesian     0.044   0.008  2.835e-03    0.017    0.009  3.185e-03  -0.072   0.019  0.007   -0.069   0.004  0.001   -0.110   0.040  0.014   -0.104    0.009  0.003           0.382          0.208  0.074           0.685           0.035  0.012     0.506    0.237  0.084     0.727     0.045  0.016      0.054      0.056  0.020       0.060      0.014  0.005\n",
       "huber        0.049   0.005  1.944e-03    0.010    0.001  4.665e-04  -0.071   0.020  0.007   -0.066   0.004  0.001   -0.107   0.040  0.014   -0.107    0.009  0.003           0.427          0.187  0.066           0.669           0.035  0.012     0.542    0.210  0.074     0.712     0.046  0.016      0.056      0.054  0.019       0.063      0.015  0.005\n",
       "theil        1.959   0.189  6.679e-02    0.007    0.002  7.878e-04  -0.075   0.025  0.009   -0.069   0.004  0.001   -0.113   0.047  0.017   -0.110    0.008  0.003           0.414          0.175  0.062           0.652           0.031  0.011     0.518    0.194  0.069     0.696     0.045  0.016      0.051      0.048  0.017       0.061      0.014  0.005"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pooled vs unpooled (no fixed effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.30s/it]/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "/home/stei509/eu_migration_forecast/.venv/lib/python3.10/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.64s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "model_names = [\n",
    "    \"linreg_pooled\",\n",
    "    \"linreg_unpooled\",\n",
    "]\n",
    "\n",
    "pooled = [True, False]\n",
    "\n",
    "models = [\n",
    "    linear_model.HuberRegressor(),\n",
    "    linear_model.HuberRegressor(),\n",
    "]\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[best_feature_combinations[\"linear\"][\"all\"]]\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test, pooled=pooled[i])\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=models[i],\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=\"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linreg_pooled</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>9.074e-04</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.302e-03</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg_unpooled</th>\n",
       "      <td>0.592</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.493e-03</td>\n",
       "      <td>-0.149</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>6.106e-04</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-1.321</td>\n",
       "      <td>1.236</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>1.274</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                fit_time               score_time                   test_mae               train_mae                   test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "                  mean    std    sem      mean     std      sem       mean    std    sem      mean    std      sem        mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "linreg_pooled     0.031   0.006  0.002    0.008    0.003  9.074e-04  -0.071   0.020  0.007   -0.066   0.004  1.302e-03   -0.107   0.040  0.014   -0.107    0.009  0.003           0.427          0.187  0.066           0.669           0.035  0.012     0.542    0.210  0.074     0.712     0.046  0.016      0.056      0.054  0.019       0.063      0.015  0.005\n",
       "linreg_unpooled   0.592   0.339  0.120    0.019    0.007  2.493e-03  -0.149   0.053  0.019   -0.010   0.002  6.106e-04   -0.205   0.078  0.027   -0.029    0.004  0.002          -1.321          1.236  0.437           0.975           0.008  0.003    -0.862    1.274  0.450     0.978     0.008  0.003     -0.022      0.033  0.012       0.119      0.018  0.006"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed v no fixed effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:07<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "model_names = [\n",
    "    \"linreg_fe\",\n",
    "    \"linreg_no_fe\",\n",
    "]\n",
    "\n",
    "fixed = [True, False]\n",
    "\n",
    "\n",
    "models = [\n",
    "    linear_model.HuberRegressor(),\n",
    "    linear_model.HuberRegressor(),\n",
    "]\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[best_feature_combinations[\"linear\"][\"all\"]]\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=models[i],\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=preprocessing.OneHotEncoder() if fixed[i] else \"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linreg_fe</th>\n",
       "      <td>0.240</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>9.504e-04</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.106</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>linreg_no_fe</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.135e-03</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fit_time               score_time                   test_mae               train_mae               test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "               mean    std    sem      mean     std      sem       mean    std    sem      mean    std    sem      mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "linreg_fe      0.240   0.253  0.089    0.018    0.003  9.504e-04  -0.074   0.021  0.008   -0.064   0.004  0.002   -0.111   0.043  0.015   -0.106    0.009  0.003           0.380          0.205  0.072           0.679           0.026  0.009     0.512    0.215  0.076     0.720     0.037  0.013      0.052      0.051  0.018       0.066      0.014  0.005\n",
       "linreg_no_fe   0.139   0.082  0.029    0.014    0.003  1.135e-03  -0.071   0.020  0.007   -0.066   0.004  0.001   -0.107   0.040  0.014   -0.107    0.009  0.003           0.427          0.187  0.066           0.669           0.035  0.012     0.542    0.210  0.074     0.712     0.046  0.016      0.056      0.054  0.019       0.063      0.015  0.005"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Gaussian Process Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:54<00:00, 27.17s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "model_names = [\n",
    "    \"gp\",\n",
    "    \"gp_norm\",\n",
    "]\n",
    "\n",
    "kernel = gaussian_process.kernels.DotProduct() + gaussian_process.kernels.WhiteKernel()\n",
    "gaussian_process.GaussianProcessRegressor(kernel=kernel, random_state=42)\n",
    "\n",
    "models = [\n",
    "    gaussian_process.GaussianProcessRegressor(kernel=kernel, random_state=42),\n",
    "    gaussian_process.GaussianProcessRegressor(kernel=kernel, random_state=42, normalize_y=True),\n",
    "]\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[best_feature_combinations[\"linear\"][\"all\"]]\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=models[i],\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=\"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gp</th>\n",
       "      <td>1.742</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gp_norm</th>\n",
       "      <td>2.808</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.104</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fit_time               score_time               test_mae               train_mae               test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod              train_r2_mod               test_delta_mae              train_delta_mae              \n",
       "          mean    std    sem      mean     std    sem     mean    std    sem      mean    std    sem      mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem      mean      std    sem        mean       std    sem        mean       std    sem  \n",
       "gp        1.742   0.835  0.295    0.032    0.014  0.005  -0.072   0.018  0.006   -0.069   0.004  0.001   -0.11    0.039  0.014   -0.104    0.009  0.003           0.371          0.224  0.079           0.688           0.034  0.012     0.498    0.254  0.09     0.73      0.045  0.016      0.054      0.056  0.02       0.06       0.014  0.005\n",
       "gp_norm   2.808   1.268  0.448    0.033    0.019  0.007  -0.072   0.018  0.006   -0.069   0.004  0.001   -0.11    0.039  0.014   -0.104    0.009  0.003           0.372          0.223  0.079           0.688           0.034  0.012     0.498    0.253  0.09     0.73      0.045  0.016      0.054      0.056  0.02       0.06       0.014  0.005"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model performance with feature combinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAINING\n",
    "\n",
    "model_names = [\"constant\", \"previous_1\", \"previous_2\"]\n",
    "lags = [[1], [1], [2]]\n",
    "regressors = [\n",
    "    dummy.DummyRegressor(strategy=\"constant\", constant=0),\n",
    "    eumf_custom_models.LinearDummyModel(np.array([1.0]), np.array([0.0])),\n",
    "    eumf_custom_models.LinearDummyModel(np.array([1.0]), np.array([0.0])),\n",
    "]\n",
    "\n",
    "\n",
    "tuners = []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {}\n",
    "\n",
    "for reg, lag in zip(regressors, lags):\n",
    "\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro, columns=[\"value\"], lags=lag, t_min=T_MIN, t_max=T_MAX,\n",
    "    )\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=reg,\n",
    "        dummy_encoder=\"drop\",\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_rmse,\n",
    "    )\n",
    "    tuners.append(tuner)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(tuner.best_estimator_, train_stacked, cv=cv_default,)\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(tuner.best_estimator_, test_stacked,)\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>constant</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.784e-04</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2.935e-03</td>\n",
       "      <td>1.038e-03</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.180</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous_1</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>5.306e-04</td>\n",
       "      <td>0.005</td>\n",
       "      <td>7.948e-04</td>\n",
       "      <td>2.810e-04</td>\n",
       "      <td>-0.070</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous_2</th>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.086e-04</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.593e-03</td>\n",
       "      <td>5.630e-04</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fit_time                   score_time                       test_mae               test_rmse               test_explained_variance               test_r2_mod               test_delta_mae              \n",
       "             mean    std      sem        mean       std        sem       mean    std    sem      mean    std    sem             mean           std    sem       mean     std    sem        mean       std    sem  \n",
       "constant     0.005   0.001  4.784e-04    0.009    2.935e-03  1.038e-03  -0.127   0.071  0.025   -0.180   0.094  0.033           0.000          0.000  0.000     0.000    0.000  0.000      0.000      0.000  0.000\n",
       "previous_1   0.004   0.002  5.306e-04    0.005    7.948e-04  2.810e-04  -0.070   0.013  0.005   -0.105   0.027  0.010           0.414          0.224  0.079     0.505    0.273  0.096      0.057      0.065  0.023\n",
       "previous_2   0.005   0.002  7.086e-04    0.007    1.593e-03  5.630e-04  -0.092   0.020  0.007   -0.139   0.047  0.017           0.107          0.201  0.071     0.231    0.305  0.108      0.034      0.056  0.020"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>r2_mod</th>\n",
       "      <th>delta_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>previous_1</th>\n",
       "      <td>-0.061</td>\n",
       "      <td>-0.086</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous_2</th>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>constant</th>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mae   rmse   explained_variance  r2_mod  delta_mae\n",
       "previous_1 -0.061 -0.086         0.107         0.215    0.009  \n",
       "previous_2 -0.068 -0.094        -0.031         0.069    0.002  \n",
       "constant   -0.070 -0.097         0.000         0.000    0.000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_test_scores(test_scores, model_names).sort_values(\"rmse\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:08<00:00, 22.96s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "\n",
    "\n",
    "feature_combinations = [\n",
    "    \"all\",\n",
    "    \"without_ar\",\n",
    "    \"without_google\",\n",
    "]\n",
    "\n",
    "params = {\"randomforestregressor__min_samples_leaf\": [7, 8, 9, 10, 11, 12, 13]}\n",
    "\n",
    "model_names = feature_combinations\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "regressor = reg = ensemble.RandomForestRegressor(random_state=42, min_samples_leaf=10)\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[\n",
    "        best_feature_combinations[\"ensemble\"][feature_combinations[i]]\n",
    "    ]\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=ensemble.RandomForestRegressor(random_state=42),\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        # dummy_encoder=\"drop\",\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>randomforestregressor__min_samples_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                randomforestregressor__min_samples_leaf\n",
       "all                               11                   \n",
       "without_ar                         9                   \n",
       "without_google                    12                   "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        k: t.best_params_\n",
    "        for k, t in zip(model_names, tuners)\n",
    "        if t is not None\n",
    "    }\n",
    ").transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.819</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.003</td>\n",
       "      <td>8.901e-04</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>1.254</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.454e-03</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>0.352</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.948e-04</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.129</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.109</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fit_time               score_time                   test_mae               train_mae               test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "                 mean    std    sem      mean     std      sem       mean    std    sem      mean    std    sem      mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "all              0.819   0.082  0.029    0.015    0.003  8.901e-04  -0.082   0.025  0.009   -0.057   0.003  0.001   -0.122   0.047  0.017   -0.097    0.009  0.003           0.328          0.120  0.043           0.724           0.058  0.020     0.442    0.205  0.072     0.759     0.067  0.024      0.045      0.051  0.018       0.072      0.016  0.006\n",
       "without_ar       1.254   0.217  0.077    0.024    0.004  1.454e-03  -0.092   0.030  0.011   -0.058   0.004  0.001   -0.134   0.046  0.016   -0.095    0.008  0.003           0.126          0.377  0.133           0.737           0.051  0.018     0.230    0.469  0.166     0.771     0.060  0.021      0.034      0.050  0.018       0.071      0.015  0.005\n",
       "without_google   0.352   0.135  0.048    0.015    0.002  7.948e-04  -0.089   0.028  0.010   -0.068   0.004  0.001   -0.129   0.049  0.017   -0.109    0.009  0.003           0.259          0.111  0.039           0.653           0.063  0.022     0.381    0.208  0.074     0.697     0.075  0.026      0.038      0.046  0.016       0.061      0.014  0.005"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>r2_mod</th>\n",
       "      <th>delta_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.100</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mae   rmse   explained_variance  r2_mod  delta_mae\n",
       "without_google -0.068 -0.094        -0.008         0.070    0.002  \n",
       "all            -0.068 -0.095        -0.025         0.053    0.002  \n",
       "without_ar     -0.072 -0.100        -0.143        -0.047   -0.002  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_test_scores(test_scores, model_names).sort_values(\"mae\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:08<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "### TRAINING\n",
    "feature_combinations = [\n",
    "    \"all\",\n",
    "    \"without_ar\",\n",
    "    \"without_google\",\n",
    "]\n",
    "\n",
    "params = {\"randomforestregressor__min_samples_leaf\": [7, 8, 9, 10, 11, 12, 13]}\n",
    "\n",
    "model_names = feature_combinations\n",
    "\n",
    "\n",
    "tuners, estimators = [], []\n",
    "cv_scores, test_scores = [], []\n",
    "train_stackeds, test_stackeds, train_unstackeds = [], [], []\n",
    "\n",
    "params = {\n",
    "    \"huberregressor__epsilon\": np.linspace(1.0, 2.0, 50),\n",
    "    \"huberregressor__alpha\": np.power(10,np.linspace(-5, -1, 50)),\n",
    "}\n",
    "\n",
    "for i, name in enumerate(tqdm(model_names)):\n",
    "    labeled = eumf_pipeline.prepare_data(\n",
    "        panel_comb_3m_macro,\n",
    "        columns=features,\n",
    "        lags=lags_default,\n",
    "        alternate_lags=alternate_lags_default,\n",
    "        t_min=T_MIN,\n",
    "        t_max=T_MAX,\n",
    "    )\n",
    "    labeled.x = labeled.x[best_feature_combinations[\"linear\"][feature_combinations[i]]]\n",
    "\n",
    "    transformed = eumf_pipeline.transform_data(labeled)\n",
    "    train, test = eumf_pipeline.split_data(\n",
    "        transformed, t_test_min=T_TEST_MIN, t_test_max=T_TEST_MAX\n",
    "    )\n",
    "    train_stacked, test_stacked = eumf_pipeline.stack_data(train, test)\n",
    "\n",
    "    train_stackeds.append(train_stacked)\n",
    "    train_unstackeds.append(train)\n",
    "    test_stackeds.append(test_stacked)\n",
    "\n",
    "    tuner = eumf_pipeline.train_reg_model(\n",
    "        train_stacked,\n",
    "        reg=linear_model.HuberRegressor(max_iter=1000),\n",
    "        extra_pipeline_steps=[preprocessing.StandardScaler()],\n",
    "        params=params,\n",
    "        scoring=eumf_eval.scorer_mae,\n",
    "        cv=cv_default,\n",
    "        # cv=cv_default,\n",
    "        dummy_encoder=\"drop\",\n",
    "        random_iterations=20,\n",
    "    )\n",
    "    estimator = tuner.best_estimator_\n",
    "    tuners.append(tuner)\n",
    "    estimators.append(estimator)\n",
    "\n",
    "    cv_score = eumf_eval.score_cv(\n",
    "        estimator, train_stacked, cv=cv_default, return_train_score=True\n",
    "    )\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "    test_score = eumf_eval.score_test(\n",
    "        estimator,\n",
    "        test_stacked,\n",
    "    )\n",
    "    test_scores.append(test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, t in zip(cv_scores, test_scores):\n",
    "    cv_scores_global.append(c)\n",
    "    test_scores_global.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>huberregressor__epsilon</th>\n",
       "      <th>huberregressor__alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>1.735</td>\n",
       "      <td>7.906e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>1.102</td>\n",
       "      <td>8.286e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>1.020</td>\n",
       "      <td>2.330e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                huberregressor__epsilon  huberregressor__alpha\n",
       "all                      1.735                 7.906e-05      \n",
       "without_ar               1.102                 8.286e-02      \n",
       "without_google           1.020                 2.330e-03      "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        k: t.best_params_\n",
    "        for k, t in zip(model_names, tuners)\n",
    "        if t is not None\n",
    "    }\n",
    ").transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">fit_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">score_time</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_rmse</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_explained_variance</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_r2_mod</th>\n",
       "      <th colspan=\"3\" halign=\"left\">test_delta_mae</th>\n",
       "      <th colspan=\"3\" halign=\"left\">train_delta_mae</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.282e-04</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.066e-04</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>0.026</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>9.240e-04</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.115</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.119</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fit_time               score_time                   test_mae               train_mae               test_rmse               train_rmse               test_explained_variance               train_explained_variance               test_r2_mod               train_r2_mod               test_delta_mae               train_delta_mae              \n",
       "                 mean    std    sem      mean     std      sem       mean    std    sem      mean    std    sem      mean    std    sem      mean     std    sem             mean           std    sem             mean            std    sem       mean     std    sem       mean      std    sem        mean       std    sem         mean       std    sem  \n",
       "all              0.033   0.007  0.002    0.008    0.002  7.282e-04  -0.071   0.019  0.007   -0.066   0.004  0.001   -0.107   0.040  0.014   -0.107    0.009  0.003           0.425          0.192  0.068           0.673           0.036  0.013     0.539    0.214  0.076     0.716     0.047  0.017      0.056      0.054  0.019       0.063      0.015  0.005\n",
       "without_ar       0.026   0.013  0.004    0.008    0.001  4.066e-04  -0.088   0.029  0.010   -0.082   0.006  0.002   -0.131   0.050  0.018   -0.135    0.010  0.004           0.184          0.292  0.103           0.486           0.034  0.012     0.338    0.329  0.116     0.547     0.059  0.021      0.038      0.044  0.016       0.047      0.012  0.004\n",
       "without_google   0.026   0.006  0.002    0.009    0.003  9.240e-04  -0.078   0.024  0.009   -0.075   0.005  0.002   -0.115   0.046  0.016   -0.119    0.010  0.004           0.375          0.221  0.078           0.596           0.044  0.016     0.466    0.245  0.087     0.646     0.055  0.020      0.049      0.050  0.018       0.055      0.014  0.005"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_cv_scores(cv_scores, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>explained_variance</th>\n",
       "      <th>r2_mod</th>\n",
       "      <th>delta_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all</th>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.297</td>\n",
       "      <td>7.360e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_google</th>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.295</td>\n",
       "      <td>8.579e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without_ar</th>\n",
       "      <td>-0.070</td>\n",
       "      <td>-0.097</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>6.731e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 mae   rmse   explained_variance  r2_mod  delta_mae\n",
       "all            -0.063 -0.082         0.257         0.297  7.360e-03\n",
       "without_google -0.062 -0.082         0.293         0.295  8.579e-03\n",
       "without_ar     -0.070 -0.097        -0.049        -0.001  6.731e-04"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eumf_eval.agg_multiple_test_scores(test_scores, model_names).sort_values(\"rmse\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a79f535b8578ad7dca5aad943b8a2492bb80ddd6fa0d945d62d9439959f4ae13"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('eumf': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
